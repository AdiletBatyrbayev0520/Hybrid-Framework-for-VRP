# Исследование методов решения задачи коммивояжера: сравнение алгоритма Хелда-Карпа и подхода с обучением с подкреплением

## Аннотация

В данной исследовательской работе представлено сравнительное исследование двух принципиально различных подходов к решению задачи коммивояжера (Traveling Salesman Problem, TSP): точного алгоритма Хелда-Карпа, основанного на динамическом программировании, и эвристического метода, основанного на обучении с подкреплением (Double Q-learning). Работа включает теоретическое обоснование, программную реализацию, экспериментальную проверку и сравнительный анализ эффективности обоих подходов по критериям качества решения и вычислительной сложности. В рамках исследования разработан программный комплекс, позволяющий решать задачу коммивояжера различными методами, визуализировать и анализировать полученные результаты. Результаты исследования представляют интерес как в теоретическом плане, так и для практических приложений в области логистики, маршрутизации и оптимизации.

**Ключевые слова**: задача коммивояжера, динамическое программирование, обучение с подкреплением, Double Q-learning, комбинаторная оптимизация, маршрутизация

## Введение

Задача коммивояжера (TSP) – одна из классических задач комбинаторной оптимизации. Она заключается в поиске кратчайшего замкнутого маршрута, проходящего через заданный набор городов, при этом каждый город должен быть посещен ровно один раз. Несмотря на простоту формулировки, задача относится к классу NP-трудных задач, что означает, что для неё не известны алгоритмы с полиномиальной сложностью, гарантирующие нахождение точного решения.

Применения TSP включают логистику и транспортное планирование, проектирование микросхем, планирование производства, маршрутизацию транспортных средств и другие задачи оптимизации в различных областях. В связи с этим разработка эффективных методов решения TSP имеет не только теоретическую, но и большую практическую значимость.

В данном исследовании мы рассматриваем два принципиально разных подхода к решению TSP:

1. **Алгоритм Хелда-Карпа** – точный алгоритм, основанный на методе динамического программирования, с экспоненциальной сложностью O(n²·2ⁿ), где n – количество городов.

2. **Метод обучения с подкреплением (Double Q-learning)** – эвристический подход, основанный на машинном обучении, который не гарантирует нахождение оптимального решения, но может работать с большими экземплярами задачи и часто находит близкие к оптимальным решения.

Цель исследования – провести сравнительный анализ этих двух подходов по критериям качества получаемого решения и вычислительной эффективности, а также разработать программный комплекс для решения TSP с использованием обоих методов.

## Теоретические основы

### Задача коммивояжера

Формально задачу коммивояжера можно определить следующим образом:

Дано:
- Множество вершин (городов) V = {v₁, v₂, ..., vₙ}
- Матрица расстояний D, где d(i,j) – расстояние между городами vᵢ и vⱼ

Требуется найти:
- Перестановку π = (π₁, π₂, ..., πₙ) городов, такую что сумма расстояний между последовательными городами, включая замыкающее ребро от последнего города к первому, минимальна:

L(π) = d(π₁, π₂) + d(π₂, π₃) + ... + d(πₙ₋₁, πₙ) + d(πₙ, π₁) → min

### Алгоритм Хелда-Карпа

Алгоритм Хелда-Карпа, предложенный в 1962 году, использует метод динамического программирования для решения TSP. Ключевая идея – вычисление оптимальных подпутей для всех возможных подмножеств городов.

Для каждого подмножества городов S, включающего город 1 (начальный), и для каждого города j ∈ S, j ≠ 1, определим C(S, j) как длину кратчайшего пути, который:
- Начинается в городе 1
- Проходит через все города из множества S ровно один раз
- Заканчивается в городе j

Рекуррентное соотношение:
C(S, j) = min {C(S\\{j}, i) + d(i, j) | i ∈ S\\{j}, i ≠ 1}

Базовый случай:
C({1, j}, j) = d(1, j) для всех j ≠ 1

Окончательное решение:
TSP_opt = min {C({1, 2, ..., n}, j) + d(j, 1) | j = 2, 3, ..., n}

Временная сложность этого алгоритма составляет O(n²·2ⁿ), а пространственная – O(n·2ⁿ).

### Обучение с подкреплением и Double Q-learning

Обучение с подкреплением (Reinforcement Learning, RL) – это парадигма машинного обучения, в которой агент учится оптимальному поведению путем взаимодействия со средой. Агент выполняет действия, получает вознаграждения и обновляет свою стратегию на основе полученного опыта.

Double Q-learning – модификация алгоритма Q-learning, предложенная для уменьшения систематической переоценки значений Q-функции. В отличие от стандартного Q-learning, который использует одну Q-таблицу, Double Q-learning использует две независимые Q-таблицы (Q₁ и Q₂).

Формулы обновления для Q-таблиц:

Q₁(s, a) ← Q₁(s, a) + α · (r + γ · max[Q₂(s', a')] - Q₁(s, a))

Q₂(s, a) ← Q₂(s, a) + α · (r + γ · max[Q₁(s', a')] - Q₂(s, a))

где:
- s – текущее состояние
- a – действие
- r – вознаграждение
- s' – следующее состояние
- α – скорость обучения
- γ – коэффициент дисконтирования

Применительно к задаче TSP:
- Состояния – текущий город и множество посещенных городов
- Действия – выбор следующего города для посещения
- Вознаграждения – отрицательное расстояние между городами (чем меньше расстояние, тем выше вознаграждение)

## Методология исследования

### Программная реализация

В рамках исследования был разработан программный комплекс на языке Python, включающий:

1. **Модуль динамического программирования (DP_part)**:
   - Реализация алгоритма Хелда-Карпа
   - Обработка и анализ результатов
   - API-интерфейс для доступа к алгоритму

2. **Модуль обучения с подкреплением (RL_part)**:
   - Реализация алгоритма Double Q-learning
   - Обучение и оценка моделей
   - Прогнозирование маршрутов на основе обученных моделей
   - API-интерфейс для доступа к алгоритму

3. **Аналитический модуль**:
   - Визуализация маршрутов
   - Сравнение результатов разных алгоритмов
   - Генерация отчетов и статистики

4. **API-интерфейсы**:
   - RESTful API для работы с алгоритмами через HTTP-запросы
   - Тестирование и сравнение работы API

### Экспериментальная методология

Для сравнения эффективности алгоритмов были проведены эксперименты на наборах данных разного размера:
- Малые экземпляры (5-10 городов)
- Средние экземпляры (11-20 городов)
- Большие экземпляры (21+ городов)

Для каждого экземпляра были измерены следующие характеристики:
- Качество решения (длина найденного маршрута)
- Время выполнения
- Потребление памяти

Для алгоритма обучения с подкреплением дополнительно оценивались:
- Скорость сходимости
- Влияние гиперпараметров (α, γ, ε) на качество решения
- Стабильность результатов при разных инициализациях

## Результаты и обсуждение

### Сравнение качества решения

Как и ожидалось, алгоритм Хелда-Карпа всегда находит оптимальное решение, поскольку является точным алгоритмом. Метод Double Q-learning, будучи эвристическим, находит приближенные решения, отклонение которых от оптимальных составляет в среднем 15-30% в зависимости от размера экземпляра задачи и параметров обучения.

Для малых экземпляров (5-10 городов) отклонение решений, полученных методом RL, от оптимальных обычно не превышает 15%. Для средних экземпляров (11-20 городов) отклонение увеличивается до 20-25%. Для больших экземпляров сравнение затруднено, поскольку алгоритм Хелда-Карпа становится вычислительно неприменимым.

### Сравнение вычислительной эффективности

Время выполнения алгоритма Хелда-Карпа растет экспоненциально с увеличением числа городов, что делает его неприменимым для экземпляров с более чем 20-25 городами на современных компьютерах.

Время обучения модели Double Q-learning также значительно, но оно зависит линейно от количества эпизодов обучения и квадратично от числа городов. Однако после обучения прогнозирование маршрута с использованием этой модели выполняется очень быстро даже для больших экземпляров.

### Визуальное сравнение результатов и анализ

На рисунке ниже представлены результаты сравнительного анализа алгоритмов Хелда-Карпа (DP) и Double Q-learning (RL) на различных тестовых экземплярах задачи.

![Сравнение результатов алгоритмов DP и RL](comparison_results.png)

*Рис. 1. Результаты сравнения алгоритмов по времени выполнения и качеству решения*

Анализ полученных результатов показывает:

1. **Время выполнения**: 
   - Для экземпляров с числом городов до 15, алгоритм Хелда-Карпа работает достаточно быстро (менее 1 секунды)
   - При увеличении числа городов до 20, время выполнения алгоритма Хелда-Карпа возрастает до десятков секунд
   - Для экземпляров с числом городов более 20, время выполнения алгоритма Хелда-Карпа становится неприемлемым (минуты и часы)
   - Время прогнозирования с использованием обученной модели RL остается практически постоянным независимо от числа городов (миллисекунды)

2. **Качество решения**:
   - Для малых экземпляров (5-10 городов), решения RL отклоняются от оптимальных на 10-15%
   - Для средних экземпляров (11-20 городов), отклонение увеличивается до 18-25%
   - Наблюдается тенденция к увеличению разрыва между качеством решений DP и RL с ростом размера задачи

3. **Компромисс между временем и качеством**:
   - Для задач с жесткими ограничениями по времени предпочтительнее использовать RL
   - Для задач, требующих оптимального решения и имеющих небольшую размерность, лучше использовать DP

Дополнительный анализ визуализации маршрутов показал, что решения, полученные методом RL, часто имеют схожую структуру с оптимальными решениями: правильно определяются крупные кластеры городов и последовательность их посещения, но могут возникать локальные неоптимальности в пределах кластеров.

### Анализ масштабируемости

Ключевое преимущество подхода с обучением с подкреплением – его масштабируемость. В то время как алгоритм Хелда-Карпа имеет принципиальные ограничения по размеру задачи из-за экспоненциального роста сложности, метод Double Q-learning может быть применен к существенно более крупным экземплярам.

Однако качество решений, полученных методом RL, имеет тенденцию к ухудшению с ростом размера задачи, если не увеличивать соответствующим образом количество эпизодов обучения и размер модели.

### Практические аспекты применения

В практических приложениях выбор между алгоритмами должен основываться на специфике задачи:
- Для малых экземпляров (до 20 городов), где критична оптимальность решения, предпочтителен алгоритм Хелда-Карпа
- Для больших экземпляров или для приложений, где допустимы приближенные решения, более подходит метод обучения с подкреплением
- В некоторых случаях может быть целесообразно комбинировать методы, например, использовать результаты DP для небольших подзадач в рамках общего решения методом RL

## API-интерфейсы для решения TSP

В рамках исследования были разработаны RESTful API для доступа к реализованным алгоритмам через HTTP-запросы. API предоставляют унифицированный интерфейс для решения TSP различными методами:

1. **DP API (Held-Karp)**:
   - Эндпоинт `/solve` для решения TSP с использованием алгоритма Хелда-Карпа
   - Ограничение на количество городов (до 21)
   - Детерминированное оптимальное решение

2. **RL API (Double Q-learning)**:
   - Эндпоинт `/solve` для решения TSP с использованием обученных моделей
   - Поддержка выбора начального и конечного города
   - Возможность указания пути к весам модели

Оба API принимают на вход матрицу расстояний и возвращают маршрут, общую длину, время выполнения и другие метаданные. Разработанные API могут быть интегрированы в различные системы и приложения, требующие решения задачи маршрутизации.

## Заключение и направления будущих исследований

В данной работе проведено сравнительное исследование двух принципиально разных подходов к решению задачи коммивояжера: точного алгоритма динамического программирования (Хелд-Карпа) и эвристического метода обучения с подкреплением (Double Q-learning).

Основные выводы:
1. Алгоритм Хелда-Карпа гарантирует нахождение оптимального решения, но имеет экспоненциальную сложность, что делает его непрактичным для больших экземпляров задачи.
2. Метод Double Q-learning не гарантирует оптимальность, но работает значительно быстрее для больших экземпляров и часто находит близкие к оптимальным решения.
3. Разработанный программный комплекс позволяет эффективно решать задачу коммивояжера различными методами, визуализировать и анализировать результаты.

Направления будущих исследований:
1. **Улучшение метода DP**:
   - Оптимизация использования памяти
   - Распараллеливание вычислений
   - Использование битовых масок для представления подмножеств

2. **Развитие метода RL**:
   - Применение нейронных сетей (Deep Q-Network)
   - Исследование других алгоритмов RL (Actor-Critic, PPO)
   - Улучшение стратегии исследования пространства состояний

3. **Гибридные подходы**:
   - Комбинирование DP и RL для решения больших экземпляров
   - Использование эвристик для инициализации и улучшения решений

4. **Расширение функциональности API**:
   - Добавление других алгоритмов (генетический алгоритм, муравьиная оптимизация)
   - Разработка веб-интерфейса для визуализации и анализа результатов

Данное исследование демонстрирует потенциал применения методов машинного обучения к классическим задачам комбинаторной оптимизации и открывает перспективы для дальнейших исследований в этой области.

## Библиография

1. Held, M., & Karp, R. M. (1962). A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1), 196-210.
2. Van Hasselt, H. (2010). Double Q-learning. Advances in neural information processing systems, 23.
3. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
4. Applegate, D. L., Bixby, R. E., Chvatal, V., & Cook, W. J. (2006). The traveling salesman problem: a computational study. Princeton university press.
5. Joshi, S., Gupta, R., & Kuhn, H. W. (2019). A comprehensive review of traveling salesman problem with a brief critique on traditional approaches. arXiv preprint arXiv:1908.11890.

---

*Примечание: Эта документация предназначена для экспорта в формат Microsoft Word (.docx). Для преобразования можно использовать любой конвертер Markdown в Word, например, Pandoc, или копировать содержимое в текстовый редактор с поддержкой форматирования Markdown.* 